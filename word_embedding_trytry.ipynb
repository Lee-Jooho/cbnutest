{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from training_trytry import *\n",
    "from training_param_add import *\n",
    "from WordInput import *\n",
    "from t_data_from_x_data import *\n",
    "from network_check import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_inform() << class\n",
    "#### wordinput() <<< 문장을 입력했을때 단어들로 구분하는 함수\n",
    "#### word_voc(voc) <<< 이전에 있는 voc라는 단어장을 사용해서 단어장에 없는 단어를 추가하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    voc\n",
    "except NameError:\n",
    "    voc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = []\n",
    "wi.inform['voc'] = []\n",
    "#초기화용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 사이에 voc를 다운 받을 수 있는 코드가 있으면 더 좋겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장을 입력해주세요 : what did you say\n"
     ]
    }
   ],
   "source": [
    "wi = word_inform()\n",
    "wi.wordinput() # 문장들을 받아올 경우 wi.inform['sentence_words']에 대문자 >> 소문자 // i'll 같은 줄임말은 풀어서 문장별로 받아온다.\n",
    "wi.word_voc(voc)# (voc) 를 보면 괄호안에 있는 voc랑 비교를 해서 이전과 비교해서 단어장에 새로운 단어가 있으면 추가한다.\n",
    "# 추가로 몇개의 단어가 추가되었는지도 확인한다.\n",
    "# 내가 만들어놓은 단어장이 아닌 특정한 단어들만 모아놓은 단어의 집합이 있다면 그 파일을 불러와서 학습시킬수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_words': [['what', 'did', 'you', 'say']],\n",
       " 'voc': ['what', 'did', 'you', 'say'],\n",
       " 'voc_length_diff': 4,\n",
       " 'voc_length': 4,\n",
       " 'word_vector': [[array([1, 0, 0, 0]),\n",
       "   array([0, 1, 0, 0]),\n",
       "   array([0, 0, 1, 0]),\n",
       "   array([0, 0, 0, 1])]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi.inform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00274907  0.01483766]\n",
      " [ 0.00263264  0.00991847]\n",
      " [-0.00332084  0.00264377]\n",
      " [-0.00096005 -0.00827901]]\n",
      "[[-0.00763586 -0.00577072  0.00064503 -0.00040445]\n",
      " [-0.01093994 -0.00149183 -0.00267558  0.00129828]]\n"
     ]
    }
   ],
   "source": [
    "net = network()\n",
    "net.net_update(wi)\n",
    "print(net.network.params['W1'])\n",
    "print(net.network.params['W2'])\n",
    "\n",
    "\n",
    "#처음 실행할때는 무조건 network를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 밑에 있는 net.net_update는 처음에 net을 선언하고나서부터는\n",
    "\n",
    "## wi.wordinput()  >>\n",
    "                                              이 온다음에 무조건 해줘야된다.\n",
    "### wi.word_voc(voc) >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장을 입력해주세요 : you say what?\n"
     ]
    }
   ],
   "source": [
    "wi.wordinput()\n",
    "wi.word_voc(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking new words ~~ing~~ \n",
      "추가된 단어 수  0\n",
      "network is ready to use\n",
      "[[ 0.00274907  0.01483766]\n",
      " [ 0.00263264  0.00991847]\n",
      " [-0.00332084  0.00264377]\n",
      " [-0.00096005 -0.00827901]]\n",
      "[[-0.00763586 -0.00577072  0.00064503 -0.00040445]\n",
      " [-0.01093994 -0.00149183 -0.00267558  0.00129828]]\n"
     ]
    }
   ],
   "source": [
    "net.net_update(wi)\n",
    "print(net.network.params['W1'])\n",
    "print(net.network.params['W2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wi.inform['voc'])\n",
    "wi.inform['voc_length_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wi.inform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서 부터는 사용하지 않는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WI = input('문장을 입력해주세요 : ') # 문장 받아오기. WI = word input.\n",
    "WI = WI.replace('\\n',' ') # 문단에 줄 내림이 있다면, 스페이스바로 바꿔주기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "be = {'am', 'is', 'are', 'be' , 'was', 'were'} #  be 동사 저장.\n",
    "\n",
    "WI = WI.lower()\n",
    "\n",
    "WI = WI.replace(\"i'm\",'i am') # be동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"he's\",'he is') # be동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"she's\",'she is') # be동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"that's\",'that is') # be동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"it's\",'it is') # be동사를 찾아내기 위해, 변환을 해준다.  (is 줄임말 풀어주기.)\n",
    "\n",
    "WI = WI.replace(\"you're\",'you are') # be동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"they're\",'they are') # be동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"we're\",'we are') # be동사를 찾아내기 위해, 변환을 해준다.\n",
    "\n",
    "Auxiliary_verb = {'will','would','can','could','shall','should','may','might','must'}\n",
    "\n",
    "WI = WI.replace(\"i'll\",'i will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"you'll\",'you will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"they'll\",'they will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"we'll\",'we will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"he'll\",'he will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"she'll\",'she will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"it'll\",'it will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"that'll\",'that will') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "\n",
    "WI = WI.replace(\"i'd\",'i would') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"you'd\",'you would') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"they'd\",'they would') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"we'd\",'we would') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"he'd\",'he would') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "WI = WI.replace(\"she'd\",'she would') # 조동사를 찾아내기 위해, 변환을 해준다.\n",
    "\n",
    "sentence = WI.strip(string.punctuation).split('.') # 문단에 마침표가 있다면, 문장을 분리해주기. 마지막에 있는 구두점 떼어주기.\n",
    "sentence_words = [s.split() for s in sentence] # 각각의 문장속에 있는 단어 분리 해주기.\n",
    "print(sentence_words) # 확인하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for length in range(len(sentence_words)):\n",
    "    for vocab in sentence_words[length]:\n",
    "        if vocab not in voc:\n",
    "            voc.append(vocab)\n",
    "\n",
    "print(voc) # 중복되지 않는 단어들을 추가해서 voc(단어장)을 만든다.\n",
    "voc_length = len(voc)\n",
    "print(voc_length) # 나중에 단어장에 단어가 추가 됬는지 안되었는지 확인해야됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_vector = np.zeros_like(sentence_words, dtype = object) ## 에러 때문에 dtype을 object로 했는데, 나중에 int로 변환해야됨. (완료됨)\n",
    "\n",
    "word_vector = [[] for i in sentence_words]\n",
    "\n",
    "# word_vector >> 입력받은 문장들을 단어별로 구분해 놓은 리스트.\n",
    "\n",
    "print(sentence_words)\n",
    "print(word_vector)\n",
    "\n",
    "for sentence in sentence_words:\n",
    "    for word in sentence:\n",
    "        voc_vector = np.zeros_like(voc, dtype = object)# 단어장 크기의 새로운 벡터를 만든다.\n",
    "        index_of_input_word = voc.index(word)\n",
    "        voc_vector[index_of_input_word] += 1 # 한단어가 단어장의 몇번 index에 있는지를 확인.\n",
    "        word_vector[sentence_words.index(sentence)].append(voc_vector)\n",
    "\n",
    "\n",
    "for i in range(len(word_vector)):\n",
    "    for j in range(len(word_vector[i])):\n",
    "        word_vector[i][j] = word_vector[i][j].astype(int) # int로 바꿔줌 ( 첫번째 줄 변환해야되는거 완료/!!)\n",
    "print(word_vector[0][0].dtype)\n",
    "print(word_vector)\n",
    "\n",
    "# 위의 두개를 가지고 딥러닝을 시키고 싶음 >> bow 방식\n",
    "# size를 정해두고 그 안에 있는 값들을 다 넣어주는 방식 >> cbow 방식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 내가 해야될 거.\n",
    "\n",
    "### 1. 한 단어를 학습 시킴에 있어서 모든 문장에 대해서 하기.\n",
    "\n",
    "예를 들어서 한 단어를 학습을 한 문장에서 하고 다음문장에서 안하는 것이 아니라, 그 단어가 들어있는 모든 단어에서 하기.\n",
    "(보류)\n",
    "\n",
    "이걸 굳이 할 필요가 있나?\n",
    "\n",
    "학습을 할때 랜덤값이 부여된 상태 (초기값)에서 1번씩만 훈련을 한다. >> 문장이 많아지면 많아질수록 정확한 값을 얻어온다.\n",
    "그럼 초기값을 다 1로 하면 되지 않을까? (초기값을 0으로 하는것은 에러)>> w값이 뭐가 곱해져도 0이 나오기 때문.\n",
    "초기값을 1/n으로 하는것은 어떤가? (n은 단어장의 크기) >> 단어장의 크기가 너무 커지면 결과적으로 0이 될 확률이 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector)\n",
    "print(sentence)\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector[x][y])\n",
    "print(t_data)\n",
    "print(tdata_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network가 있는지 확인을 안하고 변수를 생성하면 init때문에 가중치 값이 초기화가 된다.\n",
    "\n",
    "### 이것을 방지하기 위해 network가 이미 있는 경우는 생성을 안하고 없는 경우에만 생성을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi.wordinput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not network_check:\n",
    "    print(\"please make new network\")# \"network가 없으면\" True , \"network가 있으면\" false\n",
    "    voc_length = wi.inform['voc_length']\n",
    "    network = TwoLayerNet(input_size = voc_length,hidden_size = 10, output_size = voc_length)\n",
    "else:\n",
    "    print(\"checking new words ~~ing~~ \")\n",
    "    wi.word_voc(voc)\n",
    "    print(wi.inform['voc_length_diff'])\n",
    "    new_word_count = wi.inform['voc_length_diff']\n",
    "    if new_word_count == 0:\n",
    "        print(\"network is ready to use\")\n",
    "    else:\n",
    "        print(\"adding words in vocabulary\")\n",
    "        add_param = new_word_in_voc(network.params['W1'],network.params['W2'],new_word_count)\n",
    "        add_param.make_new_params()\n",
    "        print(\"ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array(word_vector[x][y])\n",
    "t_data = np.array(t_data)  # network (TwoLayerNet)에서 계산이 되게 하기 위함.\n",
    "\n",
    "train_loss_list = []\n",
    "iters_num = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size = len(x_data), hidden_size = 10, output_size = len(t_data[0]))\n",
    "for x in range(iters_num):\n",
    "    for i in range(len(t_data)):\n",
    "        grad = network.numerical_gradient(x_data,t_data[i])\n",
    "    \n",
    "        for key in ('W1','W2'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "        \n",
    "#print(train_loss_list)\n",
    "for key in ('W1','W2'):\n",
    "    print(key,network.params[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래 코드는 단어장에 새로운 단어가 추가될 경우\n",
    "## w1과 w2의 shape가 보정되어야 하므로 그것을 보정하는 코드입니다.\n",
    "### 잘 작동하는지 한번 돌려봤습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_param_add import *\n",
    "\n",
    "A = np.array([[1,2,3],[4,5,6],[7,8,9],[1,2,3]])\n",
    "B = np.array([[1,2,3,4],[6,7,8,9],[1,2,3,4]])\n",
    "\n",
    "add_param = new_word_in_voc(B,A,2)\n",
    "add_param.make_new_params()\n",
    "print(add_param.params['W1'])\n",
    "print(\"\\n\")\n",
    "print(add_param.params['W2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wi.inform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(sentence_words)):\n",
    "    for y in range(len(sentence_words[x])):\n",
    "        # y = 문장 중에서 x번째 문장에 있는 단어들의 index\n",
    "        print(len(sentence_words[x]))\n",
    "        \n",
    "        print('x = {}'.format(x)), print('y = {}'.format(y))\n",
    "        print(sentence_words[x][y])\n",
    "        print(word_vector[x][y])\n",
    "        t_data = getting_tdata(sentence_words, x, y)\n",
    "        \n",
    "        print(t_data)\n",
    "        \n",
    "        tdata_vectors =[]\n",
    "\n",
    "        for word in t_data:\n",
    "            tdata_vector = np.zeros_like(voc, dtype = int)\n",
    "            index_of_tdata_word = voc.index(word)\n",
    "            tdata_vector[index_of_tdata_word] += 1\n",
    "            \n",
    "            tdata_vectors.append(tdata_vector)\n",
    "    \n",
    "        print(tdata_vectors)\n",
    "            \n",
    "        #여기서 학습을 진행해야됨."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
